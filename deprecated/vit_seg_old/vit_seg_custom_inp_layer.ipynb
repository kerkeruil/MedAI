{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\anaconda3\\envs\\MAI_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Gebruiker\\anaconda3\\envs\\MAI_env\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from os import path\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from typing import Sequence\n",
    "from torchvision.transforms import functional as F\n",
    "import numbers\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import torchmetrics as TM\n",
    "from dataclasses import dataclass\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Send the Tensor or Model (input argument x) to the right device\n",
    "# for this notebook. i.e. if GPU is enabled, then send to GPU/CUDA\n",
    "# otherwise send to CPU.\n",
    "def to_device(x):\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cuda()\n",
    "    else:\n",
    "        return x.cpu()\n",
    "    \n",
    "def print_title(title):\n",
    "    title_len = len(title)\n",
    "    dashes = ''.join([\"-\"] * title_len)\n",
    "    print(f\"\\n{title}\\n{dashes}\")\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def find_path_to_folder(tag):\n",
    "    \"\"\"\n",
    "    Find folder that contains given tag.\n",
    "    Returns the local path to this folder.\n",
    "    \"\"\"\n",
    "    tag = str(Path(tag))\n",
    "    workdir = os.getcwd()\n",
    "    print(f'Looking for {tag} in: {workdir}')\n",
    "    n = len(tag)\n",
    "    found = False\n",
    "    for (dir_path, dir_names, file_names) in os.walk(workdir , topdown=True):\n",
    "      if tag == dir_path[-n:]:\n",
    "        print(f'Found {dir_path}\\n')\n",
    "        found = dir_path\n",
    "        break\n",
    "\n",
    "    if not found:\n",
    "        raise Exception(\"Couldn't find the folder\")\n",
    "    return Path(found)\n",
    "\n",
    "def create_slice_matrix(path_to_image, fracture, label: str):\n",
    "    \"\"\"\n",
    "    Read in all images of a given folder. Returns a matrix with all the images \n",
    "    stacked in the 3e dimension.\n",
    "    \"\"\"\n",
    "    im_size = 64\n",
    "    path_to_slices = os.path.join(path_to_image, fracture, label)\n",
    "    slices = natsorted(os.listdir(path_to_slices))\n",
    "    tmp_matrix = np.zeros((im_size, im_size, len(slices)))\n",
    "\n",
    "    for i, s in enumerate(slices):\n",
    "        pts = os.path.join(path_to_slices, s) # pts is path_to_slice  \n",
    "        tmp_matrix[:, :, i] = np.load(pts)\n",
    "\n",
    "    return tmp_matrix\n",
    "\n",
    "def readin_slices(path_to_image_folder, image_inds: list = None) -> dict:\n",
    "    \"\"\"\n",
    "    Creates and returns a dictionary with 3d numpy arrays of stacked images.\n",
    "    Works as follows:\n",
    "\n",
    "    dict[\"Index of image\"][\"Index of fracture][\"neg/pos_image/pos_label\"]\n",
    "    \n",
    "    Where the 3e dim (e.g: im[:,:,x]) is equal to the index of the slices.\n",
    "    \"\"\"\n",
    "    # Locate image folder containing the fractures.\n",
    "    all_paths = os.listdir(path_to_image_folder)\n",
    "    filenames = [(p,ind) for p in all_paths for ind in image_inds if ind in p]\n",
    "\n",
    "    d = {}\n",
    "    for name, ind in filenames:\n",
    "        d[ind] = {}\n",
    "        path_to_image = path_to_image_folder.joinpath(name)\n",
    "\n",
    "        # Iterate over fractures.\n",
    "        fracs = os.listdir(path_to_image)\n",
    "        for f in fracs:\n",
    "            neg_matrix = create_slice_matrix(path_to_image, f, 'neg')\n",
    "            pos_image_matrix = create_slice_matrix(path_to_image, f, 'pos_image')\n",
    "            pos_label_matrix = create_slice_matrix(path_to_image, f, 'pos_label')\n",
    "\n",
    "            d[ind][f] = {'neg': neg_matrix, 'pos_image': pos_image_matrix, 'pos_label': pos_label_matrix}\n",
    "    \n",
    "    return d\n",
    "\n",
    "# path_to_image = find_path_to_folder('dataset')\n",
    "d = readin_slices(Path('../dataset'), ['422', '423', '424'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = [], []\n",
    "\n",
    "for im_ind in d.keys():\n",
    "    for frac in d[im_ind].keys():\n",
    "        inputs.append(d[im_ind][frac]['pos_image'])\n",
    "        targets.append(d[im_ind][frac]['pos_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ImageToPatches\n",
      "--------------\n",
      "\n",
      "PatchEmbedding\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# ImageToPatches returns multiple flattened square patches from an\n",
    "# input image tensor.\n",
    "class ImageToPatches(nn.Module):\n",
    "    def __init__(self, image_size, patch_size):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.size()) == 4\n",
    "        y = self.unfold(x)\n",
    "        y = y.permute(0, 2, 1)\n",
    "        return y\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "print_title(\"ImageToPatches\")\n",
    "i2p = ImageToPatches(8, 4)\n",
    "\n",
    "\n",
    "# The PatchEmbedding layer takes multiple image patches in (B,T,Cin) format\n",
    "# and returns the embedded patches in (B,T,Cout) format.\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, embed_size):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_size = embed_size\n",
    "        # A single Layer is used to map all input patches to the output embedding dimension.\n",
    "        # i.e. each image patch will share the weights of this embedding layer.\n",
    "        self.embed_layer = nn.Linear(in_features=in_channels, out_features=embed_size)\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.size()) == 3\n",
    "        B, T, C = x.size()\n",
    "        x = self.embed_layer(x)\n",
    "        return x\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "print_title(\"PatchEmbedding\")\n",
    "pe = PatchEmbedding(768, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VisionTransformerInput\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "class VisionTransformerInput(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_size):\n",
    "        \"\"\"in_channels is the number of input channels in the input that will be\n",
    "        fed into this layer. For RGB images, this value would be 3.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.i2p = ImageToPatches(image_size, patch_size)\n",
    "        self.pe = PatchEmbedding(patch_size * patch_size * in_channels, embed_size)\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        # position_embed below is the learned embedding for the position of each patch\n",
    "        # in the input image. They correspond to the cosine similarity of embeddings\n",
    "        # visualized in the paper \"An Image is Worth 16x16 Words\"\n",
    "        # https://arxiv.org/pdf/2010.11929.pdf (Figure 7, Center).\n",
    "        self.position_embed = nn.Parameter(torch.randn(num_patches, embed_size))\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.i2p(x)\n",
    "        # print(x.shape)\n",
    "        x = self.pe(x)\n",
    "        x = x + self.position_embed\n",
    "        return x\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "print_title(\"VisionTransformerInput\")\n",
    "# Original\n",
    "# x = torch.randn(10, 3, 224, 224)\n",
    "# Custom\n",
    "# x = torch.randn(10, 1, 224, 224)\n",
    "\n",
    "# Original\n",
    "# vti = VisionTransformerInput(224, 16, 3, 256)\n",
    "# Custom\n",
    "vti = VisionTransformerInput(224, 16, 1, 256)\n",
    "# y = vti(x)\n",
    "# print(f\"{x.shape} -> {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision transformer segmentation building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MultiLayerPerceptron\n",
      "--------------------\n",
      "\n",
      "SelfAttentionEncoderBlock\n",
      "-------------------------\n",
      "\n",
      "OutputProjection\n",
      "----------------\n",
      "\n",
      "VisionTransformerForSegmentation\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# The MultiLayerPerceptron is a unit of computation. It expands the input\n",
    "# to 4x the number of channels, and then contracts it back into the number\n",
    "# of input channels. There's a GeLU activation in between, and the layer\n",
    "# is followed by a droput layer.\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, embed_size, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_size * 4, embed_size),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "print_title(\"MultiLayerPerceptron\")\n",
    "mlp = MultiLayerPerceptron(60, dropout=0.2)\n",
    "\n",
    "\n",
    "# This is a single self-attention encoder block, which has a multi-head attention\n",
    "# block within it. The MultiHeadAttention block performs communication, while the\n",
    "# MultiLayerPerceptron performs computation.\n",
    "class SelfAttentionEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        # self.kqv = nn.Linear(embed_size, embed_size * 3)\n",
    "        self.mha = nn.MultiheadAttention(embed_size, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "        self.mlp = MultiLayerPerceptron(embed_size, dropout)\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.ln1(x)\n",
    "        # y = self.kqv(x)\n",
    "        # (q, k, v) = torch.split(y, self.embed_size, dim=2)\n",
    "        x = x + self.mha(y, y, y, need_weights=False)[0]\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "print_title(\"SelfAttentionEncoderBlock\")\n",
    "attention_block = SelfAttentionEncoderBlock(256, 8, dropout=0.2)\n",
    "\n",
    "\n",
    "# Similar to the PatchEmbedding class, we need to un-embed the representation\n",
    "# of each patch that has been produced by our transformer network. We project\n",
    "# each patch (that has embed_size) dimensions into patch_size*patch_size*output_dims\n",
    "# channels, and then fold all the patches back to make it look like an image.\n",
    "class OutputProjection(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, embed_size, output_dims):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.output_dims = output_dims\n",
    "        self.projection = nn.Linear(embed_size, patch_size * patch_size * output_dims)\n",
    "        self.fold = nn.Fold(output_size=(image_size, image_size), kernel_size=patch_size, stride=patch_size)\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = self.projection(x)\n",
    "        # x will now have shape (B, T, PatchSize**2 * OutputDims). This can be folded into\n",
    "        # the desired output shape.\n",
    "\n",
    "        # To fold the patches back into an image-like form, we need to first\n",
    "        # swap the T and C dimensions to make it a (B, C, T) tensor.\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fold(x)\n",
    "        return x\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "print_title(\"OutputProjection\")\n",
    "# Original\n",
    "# op = OutputProjection(224, 16, 256, 3)\n",
    "# Custom\n",
    "op = OutputProjection(224, 16, 256, 1)\n",
    "\n",
    "\n",
    "class VisionTransformerForSegmentation(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, out_channels, embed_size, num_blocks, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        heads = [ SelfAttentionEncoderBlock(embed_size, num_heads, dropout) for i in range(num_blocks) ]\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=in_channels),\n",
    "            VisionTransformerInput(image_size, patch_size, in_channels, embed_size),\n",
    "            nn.Sequential(*heads),\n",
    "            OutputProjection(image_size, patch_size, embed_size, out_channels),\n",
    "        )\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "@dataclass\n",
    "class VisionTransformerArgs:\n",
    "    \"\"\"Arguments to the VisionTransformerForSegmentation.\"\"\"\n",
    "    image_size: int = 128\n",
    "    patch_size: int = 16\n",
    "    in_channels: int = 1 # Number of layers in image (RGB=3), set to 1 for grayscale\n",
    "    out_channels: int = 1 # N layers that should be put out eventually by the network, set to 1 for grayscale again\n",
    "    embed_size: int = 768\n",
    "    num_blocks: int = 12\n",
    "    num_heads: int = 8\n",
    "    dropout: float = 0.2\n",
    "# end class\n",
    "\n",
    "print_title(\"VisionTransformerForSegmentation\")\n",
    "# x = torch.randn(2, 1, 128, 128)\n",
    "vit_args = dataclasses.asdict(VisionTransformerArgs())\n",
    "\n",
    "vit = VisionTransformerForSegmentation(**vit_args)\n",
    "# y = vit(x)\n",
    "# print(f\"{x.shape} -> {y.shape}\")\n",
    "# print_model_parameters(vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6861, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a custom IoU Metric for validating the model.\n",
    "def IoUMetric(pred, gt, softmax=False):\n",
    "    # Run softmax if input is logits.\n",
    "    if softmax is True:\n",
    "        pred = nn.Softmax(dim=1)(pred)\n",
    "    # end if\n",
    "\n",
    "    # Add the one-hot encoded masks for all 3 output channels\n",
    "    # (for all the classes) to a tensor named 'gt' (ground truth).\n",
    "    gt = torch.cat([ (gt == i) for i in range(3) ], dim=1)\n",
    "    # print(f\"[2] Pred shape: {pred.shape}, gt shape: {gt.shape}\")\n",
    "\n",
    "    intersection = gt * pred\n",
    "    union = gt + pred - intersection\n",
    "\n",
    "    # Compute the sum over all the dimensions except for the batch dimension.\n",
    "    iou = (intersection.sum(dim=(1, 2, 3)) + 0.001) / (union.sum(dim=(1, 2, 3)) + 0.001)\n",
    "\n",
    "    # Compute the mean over the batch dimension.\n",
    "    return iou.mean()\n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, softmax=False):\n",
    "        super().__init__()\n",
    "        self.softmax = softmax\n",
    "\n",
    "    # pred => Predictions (logits, B, 3, H, W)\n",
    "    # gt => Ground Truth Labales (B, 1, H, W)\n",
    "    def forward(self, pred, gt):\n",
    "        # return 1.0 - IoUMetric(pred, gt, self.softmax)\n",
    "        # Compute the negative log loss for stable training.\n",
    "        return -(IoUMetric(pred, gt, self.softmax).log())\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "def test_custom_iou_loss():\n",
    "    #               B, C, H, W\n",
    "    x = torch.rand((2, 3, 2, 2), requires_grad=True)\n",
    "    y = torch.randint(0, 3, (2, 1, 2, 2), dtype=torch.long)\n",
    "    z = IoULoss(softmax=True)(x, y)\n",
    "    return z\n",
    "# end def\n",
    "\n",
    "test_custom_iou_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for a single epoch\n",
    "def train_model(model, loader, optimizer):\n",
    "    to_device(model.train())\n",
    "    cel = True\n",
    "    if cel:\n",
    "        criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    else:\n",
    "        criterion = IoULoss(softmax=True) # Hasn't be adjusted for 2D images jochem\n",
    "    # end if\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_samples = 0\n",
    "\n",
    "    inputs, targets = loader\n",
    "    for batch_idx, (inps, targs) in enumerate(zip(inputs, targets), 0):\n",
    "        optimizer.zero_grad()\n",
    "        inps = torch.from_numpy(inps)\n",
    "        targs = torch.from_numpy(targs)\n",
    "        # targets = targets.type(torch.long)\n",
    "\n",
    "        inps = to_device(inps)\n",
    "        targs = to_device(targs)\n",
    "        outputs = model(inps)\n",
    "\n",
    "        # The ground truth labels have a channel dimension (NCHW).\n",
    "        # We need to remove it before passing it into\n",
    "        # CrossEntropyLoss so that it has shape (NHW) and each element\n",
    "        # is a value representing the class of the pixel.\n",
    "        targs = targs.type(torch.long)\n",
    "        if cel:\n",
    "            targs = targs.squeeze(dim=1)\n",
    "        # end if\n",
    "        unqs = targs.unique()\n",
    "        print(\"UNGKS:\", unqs)\n",
    "        print(\"Targests:\", targs.shape)\n",
    "        print(\"Outputs:\", outputs.shape)\n",
    "        value = outputs[0,0,0,0].item()\n",
    "        print(\"Value in the output:\", value)\n",
    "        print(\"output type:\", type(value), '\\n')\n",
    "\n",
    "        value = targs[0,0,0].item()\n",
    "        print(\"Value in the target:\", value)\n",
    "        print(\"target shape:\", targs.shape)\n",
    "        print(\"Value type:\", type(value))\n",
    "\n",
    "        # sys.exit()\n",
    "        loss = criterion(outputs, targs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_samples += targs.size(0)\n",
    "        running_loss += loss.item()\n",
    "    # end for\n",
    "\n",
    "    print(\"Trained {} samples, Loss: {:.4f}\".format(\n",
    "        running_samples,\n",
    "        running_loss / (batch_idx+1),\n",
    "    ))\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define training loop. This will train the model for multiple epochs.\n",
    "#\n",
    "# epochs: A tuple containing the start epoch (inclusive) and end epoch (exclusive).\n",
    "#         The model is trained for [epoch[0] .. epoch[1]) epochs.\n",
    "#\n",
    "def train_loop(model, loader, test_data, epochs, optimizer, scheduler, save_path):\n",
    "    test_inputs, test_targets = test_data\n",
    "    epoch_i, epoch_j = epochs\n",
    "    for i in range(epoch_i, epoch_j):\n",
    "        epoch = i\n",
    "        print(f\"Epoch: {i:02d}, Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "        train_model(model, loader, optimizer)\n",
    "        with torch.inference_mode():\n",
    "        #     # Display the plt in the final training epoch.\n",
    "        #     # (epoch == epoch_j-1)\n",
    "            custom_test = True\n",
    "            if i == 15:\n",
    "                print(\"got here\")\n",
    "\n",
    "                to_device(model.eval())\n",
    "                inputs, targets = loader\n",
    "                for batch_idx, (inps, targs) in enumerate(zip(inputs, targets), 0):\n",
    "                    inps = torch.from_numpy(inps)\n",
    "                    inps = to_device(inps)\n",
    "                    preds = model(inps)\n",
    "                    pred = nn.Softmax(dim=1)(preds)\n",
    "                    print(\"PREDS:\", torch.unique(pred))\n",
    "                    print(\"PREDS\", pred.shape)\n",
    "                break\n",
    "        #     print_test_dataset_masks(model, test_inputs, test_targets, epoch=epoch, save_path=save_path, show_plot=True)\n",
    "        # # end with\n",
    "\n",
    "        \n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        # end if\n",
    "        print(\"\")\n",
    "    # end for\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = vit\n",
    "images_folder_name = \"vit_training_progress_images\"\n",
    "save_path = os.path.join('output', images_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.VisionTransformerForSegmentation"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Learning Rate Scheduler.\n",
    "to_device(m)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.0004)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=12, gamma=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original:\n",
    "\"\"\"\n",
    "train_loop(m, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 51), optimizer, scheduler, save_path=save_path)\n",
    "m = model\n",
    "pets_train_loader = torch dataloader object\n",
    "We will split this into a inputs and targets vars for the sake of fast implementation.\n",
    "\n",
    "test_pets_inputs = Used for unnecesary things and removed from training loop\n",
    "test_pets_targets = Used for unnecesary things and removed from training loop\n",
    "\"\"\"\n",
    "\n",
    "test_pets_inputs = None\n",
    "test_pets_targets = None\n",
    "\n",
    "# Custom:\n",
    "# train_loop(m, (inputs, targets), (test_pets_inputs, test_pets_targets), (1, 51), optimizer, scheduler, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surround_the_ones(m_arr):\n",
    "    updated_arr = m_arr.clone()\n",
    "\n",
    "    # Get the indices of ones in the 'test' tensor\n",
    "    ones_indices = torch.nonzero(m_arr == 1)\n",
    "\n",
    "    # Define the neighborhood indices for the surrounding elements\n",
    "    neighborhood_indices = torch.tensor([\n",
    "        [1, 0], [-1, 0], [0, 1], [0, -1],\n",
    "        [1, 1], [-1, -1], [1, -1], [-1, 1]\n",
    "    ])\n",
    "    # Place ones next to twos only if there is a zero\n",
    "    for index in ones_indices:\n",
    "        updated_arr[tuple(index)] = 2  # Change the one to two\n",
    "        for offset in neighborhood_indices:\n",
    "            neighbor_index = index + offset\n",
    "            if (0 <= neighbor_index[0] < m_arr.shape[0] and\n",
    "                0 <= neighbor_index[1] < m_arr.shape[1] and\n",
    "                updated_arr[tuple(neighbor_index)] == 0):\n",
    "                updated_arr[tuple(neighbor_index)] = 1  # Place ones only if the neighbor is zero\n",
    "\n",
    "    # Print the updated 'test' tensor\n",
    "    # print(\"Updated 'test' tensor:\")\n",
    "    # print(updated_arr)\n",
    "    return updated_arr\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 1, 128, 128)\n",
      "(21, 1, 128, 128)\n",
      "Epoch: 01, Learning Rate: 0.0004\n",
      "UNGKS: tensor([0, 1, 2])\n",
      "Targests: torch.Size([21, 128, 128])\n",
      "Outputs: torch.Size([21, 1, 128, 128])\n",
      "Value in the output: -0.7524198293685913\n",
      "output type: <class 'float'> \n",
      "\n",
      "Value in the target: 0\n",
      "target shape: torch.Size([21, 128, 128])\n",
      "Value type: <class 'int'>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gebruiker\\Desktop\\MedAI\\ViT\\vit_seg_custom_inp_layer.ipynb Cell 30\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(inputs_batch\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# print(inputs_batch.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# sys.exit()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m train_loop(m, ([inputs_batch], [segmask_batch]), (test_pets_inputs, test_pets_targets), (\u001b[39m1\u001b[39;49m, \u001b[39m51\u001b[39;49m), optimizer, scheduler, save_path\u001b[39m=\u001b[39;49msave_path)\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\Desktop\\MedAI\\ViT\\vit_seg_custom_inp_layer.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m epoch \u001b[39m=\u001b[39m i\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m:\u001b[39;00m\u001b[39m02d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Learning Rate: \u001b[39m\u001b[39m{\u001b[39;00moptimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_model(model, loader, optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#     # Display the plt in the final training epoch.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#     # (epoch == epoch_j-1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     custom_test \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\Desktop\\MedAI\\ViT\\vit_seg_custom_inp_layer.ipynb Cell 30\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mValue type:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m(value))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# sys.exit()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Desktop/MedAI/ViT/vit_seg_custom_inp_layer.ipynb#X36sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\envs\\MAI_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\envs\\MAI_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\anaconda3\\envs\\MAI_env\\Lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 1 is out of bounds."
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms \n",
    "from PIL import ImageOps\n",
    "\n",
    "img_size_custom = 128\n",
    "examples = 21\n",
    "\n",
    "inputs = Image.open(\"../raw_data/test/dikke_kippen/kip2.jpg\")\n",
    "\n",
    "inputs = ImageOps.grayscale(inputs) \n",
    "\n",
    "segmask = np.load(\"../dataset/RibFrac421-image/frac_0/pos_label/pos-slice-0-label.npy\")\n",
    "segmask = torch.from_numpy(segmask)\n",
    "img2tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "\n",
    "img_tensor = img2tensor(inputs)\n",
    "img_tensor = img_tensor[:, 0:128, 0:128]\n",
    "\n",
    "# large_tensor = torch.rand(100, 100)  # A large tensor for demonstration\n",
    "\n",
    "# # Set the print options to display the entire tensor and make it wider\n",
    "# torch.set_printoptions(threshold=large_tensor.numel(), linewidth=200)\n",
    "\n",
    "segmask= surround_the_ones(segmask)\n",
    "\n",
    "# print(\"UNIQUES:\", torch.unique(segmask))\n",
    "# # sys.exit()\n",
    "# # Set the print options to display the entire tensor\n",
    "# print(torch.unique(segmask_test))\n",
    "# print(segmask_test.shape)\n",
    "# print(segmask_test)\n",
    "# for row in segmask_test:\n",
    "    # print(row)\n",
    "# sys.exit()\n",
    "\n",
    "inputs_batch = torch.zeros((examples, 1, img_size_custom, img_size_custom))\n",
    "for i in range(examples):\n",
    "    inputs_batch[i,:,:,:] = img_tensor\n",
    "\n",
    "tmp = torch.zeros((img_size_custom, img_size_custom))\n",
    "tmp[0:64, 0:64] = segmask\n",
    "segmask_batch = torch.zeros((examples, 1, img_size_custom, img_size_custom))\n",
    "for i in range(examples):\n",
    "    segmask_batch[i,:,:,:] = tmp\n",
    "\n",
    "inputs_batch = inputs_batch.numpy()\n",
    "segmask_batch = segmask_batch.numpy()\n",
    "\n",
    "print(segmask_batch.shape)\n",
    "\n",
    "print(inputs_batch.shape)\n",
    "# print(inputs_batch.shape)\n",
    "# sys.exit()\n",
    "\n",
    "\n",
    "train_loop(m, ([inputs_batch], [segmask_batch]), (test_pets_inputs, test_pets_targets), (1, 51), optimizer, scheduler, save_path=save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
