{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\anaconda3\\envs\\MAI_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Gebruiker\\anaconda3\\envs\\MAI_env\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from os import path\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from typing import Sequence\n",
    "from torchvision.transforms import functional as F\n",
    "import numbers\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import torchmetrics as TM\n",
    "from dataclasses import dataclass\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Send the Tensor or Model (input argument x) to the right device\n",
    "# for this notebook. i.e. if GPU is enabled, then send to GPU/CUDA\n",
    "# otherwise send to CPU.\n",
    "def to_device(x):\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cuda()\n",
    "    else:\n",
    "        return x.cpu()\n",
    "    \n",
    "def print_title(title):\n",
    "    title_len = len(title)\n",
    "    dashes = ''.join([\"-\"] * title_len)\n",
    "    print(f\"\\n{title}\\n{dashes}\")\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def find_path_to_folder(tag):\n",
    "    \"\"\"\n",
    "    Find folder that contains given tag.\n",
    "    Returns the local path to this folder.\n",
    "    \"\"\"\n",
    "    tag = str(Path(tag))\n",
    "    workdir = os.getcwd()\n",
    "    print(f'Looking for {tag} in: {workdir}')\n",
    "    n = len(tag)\n",
    "    found = False\n",
    "    for (dir_path, dir_names, file_names) in os.walk(workdir , topdown=True):\n",
    "      if tag == dir_path[-n:]:\n",
    "        print(f'Found {dir_path}\\n')\n",
    "        found = dir_path\n",
    "        break\n",
    "\n",
    "    if not found:\n",
    "        raise Exception(\"Couldn't find the folder\")\n",
    "    return Path(found)\n",
    "\n",
    "def create_slice_matrix(path_to_image, fracture, label: str):\n",
    "    \"\"\"\n",
    "    Read in all images of a given folder. Returns a matrix with all the images \n",
    "    stacked in the 3e dimension.\n",
    "    \"\"\"\n",
    "    im_size = 64\n",
    "    path_to_slices = os.path.join(path_to_image, fracture, label)\n",
    "    slices = natsorted(os.listdir(path_to_slices))\n",
    "    tmp_matrix = np.zeros((im_size, im_size, len(slices)))\n",
    "\n",
    "    for i, s in enumerate(slices):\n",
    "        pts = os.path.join(path_to_slices, s) # pts is path_to_slice  \n",
    "        tmp_matrix[:, :, i] = np.load(pts)\n",
    "\n",
    "    return tmp_matrix\n",
    "\n",
    "def readin_slices(path_to_image_folder, image_inds: list = None) -> dict:\n",
    "    \"\"\"\n",
    "    Creates and returns a dictionary with 3d numpy arrays of stacked images.\n",
    "    Works as follows:\n",
    "\n",
    "    dict[\"Index of image\"][\"Index of fracture][\"neg/pos_image/pos_label\"]\n",
    "    \n",
    "    Where the 3e dim (e.g: im[:,:,x]) is equal to the index of the slices.\n",
    "    \"\"\"\n",
    "    # Locate image folder containing the fractures.\n",
    "    all_paths = os.listdir(path_to_image_folder)\n",
    "    filenames = [(p,ind) for p in all_paths for ind in image_inds if ind in p]\n",
    "\n",
    "    d = {}\n",
    "    for name, ind in filenames:\n",
    "        d[ind] = {}\n",
    "        path_to_image = path_to_image_folder.joinpath(name)\n",
    "\n",
    "        # Iterate over fractures.\n",
    "        fracs = os.listdir(path_to_image)\n",
    "        for f in fracs:\n",
    "            neg_matrix = create_slice_matrix(path_to_image, f, 'neg')\n",
    "            pos_image_matrix = create_slice_matrix(path_to_image, f, 'pos_image')\n",
    "            pos_label_matrix = create_slice_matrix(path_to_image, f, 'pos_label')\n",
    "\n",
    "            d[ind][f] = {'neg': neg_matrix, 'pos_image': pos_image_matrix, 'pos_label': pos_label_matrix}\n",
    "    \n",
    "    return d\n",
    "\n",
    "# path_to_image = find_path_to_folder('dataset')\n",
    "d = readin_slices(Path('../dataset'), ['422', '423', '424'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = [], []\n",
    "\n",
    "for im_ind in d.keys():\n",
    "    for frac in d[im_ind].keys():\n",
    "        inputs.append(d[im_ind][frac]['pos_image'])\n",
    "        targets.append(d[im_ind][frac]['pos_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ImageToPatches\n",
      "--------------\n",
      "\n",
      "PatchEmbedding\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# ImageToPatches returns multiple flattened square patches from an\n",
    "# input image tensor.\n",
    "class ImageToPatches(nn.Module):\n",
    "    def __init__(self, image_size, patch_size):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.size()) == 4\n",
    "        y = self.unfold(x)\n",
    "        y = y.permute(0, 2, 1)\n",
    "        return y\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "print_title(\"ImageToPatches\")\n",
    "i2p = ImageToPatches(8, 4)\n",
    "\n",
    "\n",
    "# The PatchEmbedding layer takes multiple image patches in (B,T,Cin) format\n",
    "# and returns the embedded patches in (B,T,Cout) format.\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, embed_size):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_size = embed_size\n",
    "        # A single Layer is used to map all input patches to the output embedding dimension.\n",
    "        # i.e. each image patch will share the weights of this embedding layer.\n",
    "        self.embed_layer = nn.Linear(in_features=in_channels, out_features=embed_size)\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.size()) == 3\n",
    "        B, T, C = x.size()\n",
    "        x = self.embed_layer(x)\n",
    "        return x\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "print_title(\"PatchEmbedding\")\n",
    "pe = PatchEmbedding(768, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VisionTransformerInput\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "class VisionTransformerInput(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_size):\n",
    "        \"\"\"in_channels is the number of input channels in the input that will be\n",
    "        fed into this layer. For RGB images, this value would be 3.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.i2p = ImageToPatches(image_size, patch_size)\n",
    "        self.pe = PatchEmbedding(patch_size * patch_size * in_channels, embed_size)\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        # position_embed below is the learned embedding for the position of each patch\n",
    "        # in the input image. They correspond to the cosine similarity of embeddings\n",
    "        # visualized in the paper \"An Image is Worth 16x16 Words\"\n",
    "        # https://arxiv.org/pdf/2010.11929.pdf (Figure 7, Center).\n",
    "        self.position_embed = nn.Parameter(torch.randn(num_patches, embed_size))\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.i2p(x)\n",
    "        # print(x.shape)\n",
    "        x = self.pe(x)\n",
    "        x = x + self.position_embed\n",
    "        return x\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "print_title(\"VisionTransformerInput\")\n",
    "# Original\n",
    "x = torch.randn(10, 3, 224, 224)\n",
    "# Custom\n",
    "# x = torch.randn(10, 1, 224, 224)\n",
    "\n",
    "# Original\n",
    "vti = VisionTransformerInput(224, 16, 3, 256)\n",
    "# Custom\n",
    "# vti = VisionTransformerInput(224, 16, 1, 256)\n",
    "y = vti(x)\n",
    "# print(f\"{x.shape} -> {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision transformer segmentation building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MultiLayerPerceptron\n",
      "--------------------\n",
      "\n",
      "SelfAttentionEncoderBlock\n",
      "-------------------------\n",
      "\n",
      "OutputProjection\n",
      "----------------\n",
      "\n",
      "VisionTransformerForSegmentation\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# The MultiLayerPerceptron is a unit of computation. It expands the input\n",
    "# to 4x the number of channels, and then contracts it back into the number\n",
    "# of input channels. There's a GeLU activation in between, and the layer\n",
    "# is followed by a droput layer.\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, embed_size, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_size * 4, embed_size),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "print_title(\"MultiLayerPerceptron\")\n",
    "mlp = MultiLayerPerceptron(60, dropout=0.2)\n",
    "\n",
    "\n",
    "# This is a single self-attention encoder block, which has a multi-head attention\n",
    "# block within it. The MultiHeadAttention block performs communication, while the\n",
    "# MultiLayerPerceptron performs computation.\n",
    "class SelfAttentionEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        # self.kqv = nn.Linear(embed_size, embed_size * 3)\n",
    "        self.mha = nn.MultiheadAttention(embed_size, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "        self.mlp = MultiLayerPerceptron(embed_size, dropout)\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.ln1(x)\n",
    "        # y = self.kqv(x)\n",
    "        # (q, k, v) = torch.split(y, self.embed_size, dim=2)\n",
    "        x = x + self.mha(y, y, y, need_weights=False)[0]\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "print_title(\"SelfAttentionEncoderBlock\")\n",
    "attention_block = SelfAttentionEncoderBlock(256, 8, dropout=0.2)\n",
    "\n",
    "\n",
    "# Similar to the PatchEmbedding class, we need to un-embed the representation\n",
    "# of each patch that has been produced by our transformer network. We project\n",
    "# each patch (that has embed_size) dimensions into patch_size*patch_size*output_dims\n",
    "# channels, and then fold all the pathces back to make it look like an image.\n",
    "class OutputProjection(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, embed_size, output_dims):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.output_dims = output_dims\n",
    "        self.projection = nn.Linear(embed_size, patch_size * patch_size * output_dims)\n",
    "        self.fold = nn.Fold(output_size=(image_size, image_size), kernel_size=patch_size, stride=patch_size)\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = self.projection(x)\n",
    "        # x will now have shape (B, T, PatchSize**2 * OutputDims). This can be folded into\n",
    "        # the desired output shape.\n",
    "\n",
    "        # To fold the patches back into an image-like form, we need to first\n",
    "        # swap the T and C dimensions to make it a (B, C, T) tensor.\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fold(x)\n",
    "        return x\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "print_title(\"OutputProjection\")\n",
    "op = OutputProjection(224, 16, 256, 3)\n",
    "\n",
    "class VisionTransformerForSegmentation(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, out_channels, embed_size, num_blocks, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        heads = [ SelfAttentionEncoderBlock(embed_size, num_heads, dropout) for i in range(num_blocks) ]\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=in_channels),\n",
    "            VisionTransformerInput(image_size, patch_size, in_channels, embed_size),\n",
    "            nn.Sequential(*heads),\n",
    "            OutputProjection(image_size, patch_size, embed_size, out_channels),\n",
    "        )\n",
    "    # end def\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "@dataclass\n",
    "class VisionTransformerArgs:\n",
    "    \"\"\"Arguments to the VisionTransformerForSegmentation.\"\"\"\n",
    "    image_size: int = 128\n",
    "    patch_size: int = 16\n",
    "    in_channels: int = 3 # Number of layers in image (RGB=3), set to 1 jochem\n",
    "    out_channels: int = 3# Don't know what this does, maybe needs to be 1 as well? jochem\n",
    "    embed_size: int = 768\n",
    "    num_blocks: int = 12\n",
    "    num_heads: int = 8\n",
    "    dropout: float = 0.2\n",
    "# end class\n",
    "\n",
    "print_title(\"VisionTransformerForSegmentation\")\n",
    "vit_args = dataclasses.asdict(VisionTransformerArgs())\n",
    "\n",
    "vit = VisionTransformerForSegmentation(**vit_args)\n",
    "# print_model_parameters(vit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5530, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a custom IoU Metric for validating the model.\n",
    "def IoUMetric(pred, gt, softmax=False):\n",
    "    # Run softmax if input is logits.\n",
    "    if softmax is True:\n",
    "        pred = nn.Softmax(dim=1)(pred)\n",
    "    # end if\n",
    "\n",
    "    # Add the one-hot encoded masks for all 3 output channels\n",
    "    # (for all the classes) to a tensor named 'gt' (ground truth).\n",
    "    gt = torch.cat([ (gt == i) for i in range(3) ], dim=1)\n",
    "    # print(f\"[2] Pred shape: {pred.shape}, gt shape: {gt.shape}\")\n",
    "\n",
    "    intersection = gt * pred\n",
    "    union = gt + pred - intersection\n",
    "\n",
    "    # Compute the sum over all the dimensions except for the batch dimension.\n",
    "    iou = (intersection.sum(dim=(1, 2, 3)) + 0.001) / (union.sum(dim=(1, 2, 3)) + 0.001)\n",
    "\n",
    "    # Compute the mean over the batch dimension.\n",
    "    return iou.mean()\n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, softmax=False):\n",
    "        super().__init__()\n",
    "        self.softmax = softmax\n",
    "\n",
    "    # pred => Predictions (logits, B, 3, H, W)\n",
    "    # gt => Ground Truth Labales (B, 1, H, W)\n",
    "    def forward(self, pred, gt):\n",
    "        # return 1.0 - IoUMetric(pred, gt, self.softmax)\n",
    "        # Compute the negative log loss for stable training.\n",
    "        return -(IoUMetric(pred, gt, self.softmax).log())\n",
    "    # end def\n",
    "# end class\n",
    "\n",
    "def test_custom_iou_loss():\n",
    "    #               B, C, H, W\n",
    "    x = torch.rand((2, 3, 2, 2), requires_grad=True)\n",
    "    y = torch.randint(0, 3, (2, 1, 2, 2), dtype=torch.long)\n",
    "    z = IoULoss(softmax=True)(x, y)\n",
    "    return z\n",
    "# end def\n",
    "\n",
    "test_custom_iou_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for a single epoch\n",
    "def train_model(model, loader, optimizer):\n",
    "    to_device(model.train())\n",
    "    cel = True\n",
    "    if cel:\n",
    "        criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    else:\n",
    "        criterion = IoULoss(softmax=True)\n",
    "    # end if\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_samples = 0\n",
    "\n",
    "    inputs, targets = loader\n",
    "    for batch_idx, (inputs, targets) in enumerate(zip(inputs, targets), 0):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = torch.from_numpy(inputs)\n",
    "        targets = torch.from_numpy(targets)\n",
    "        targets = targets.type(torch.long)\n",
    "\n",
    "        inputs = to_device(inputs)\n",
    "        targets = to_device(targets)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # The ground truth labels have a channel dimension (NCHW).\n",
    "        # We need to remove it before passing it into\n",
    "        # CrossEntropyLoss so that it has shape (NHW) and each element\n",
    "        # is a value representing the class of the pixel.\n",
    "\n",
    "        # sys.exit()\n",
    "        if cel:\n",
    "            targets = targets.squeeze(dim=1)\n",
    "        # end if\n",
    "\n",
    "        # print(type(targets[0]))\n",
    "        # print(type(targets[0,0,0]))\n",
    "        # print(targets[0,0,0])\n",
    "        # print(\"Before outputs:\", outputs.shape)\n",
    "        # # outputs = outputs.squeeze(1)\n",
    "        # print(\"After outputs:\", outputs.shape)\n",
    "\n",
    "        # print(\"CHECKEN TYPE IN TENSOR:\")\n",
    "        # value = outputs[0,0,0,0].item()\n",
    "        # print(\"Value in ouput:\", type(value))\n",
    "\n",
    "        # for t in outputs:\n",
    "        #     print(t)\n",
    "    \n",
    "        # value = targets[0,0,0].item()\n",
    "        # print(\"Value in target:\", type(value))\n",
    "\n",
    "        # sys.exit\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_samples += targets.size(0)\n",
    "        running_loss += loss.item()\n",
    "    # end for\n",
    "\n",
    "    print(\"Trained {} samples, Loss: {:.4f}\".format(\n",
    "        running_samples,\n",
    "        running_loss / (batch_idx+1),\n",
    "    ))\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define training loop. This will train the model for multiple epochs.\n",
    "#\n",
    "# epochs: A tuple containing the start epoch (inclusive) and end epoch (exclusive).\n",
    "#         The model is trained for [epoch[0] .. epoch[1]) epochs.\n",
    "#\n",
    "def train_loop(model, loader, test_data, epochs, optimizer, scheduler, save_path):\n",
    "    test_inputs, test_targets = test_data\n",
    "    epoch_i, epoch_j = epochs\n",
    "    for i in range(epoch_i, epoch_j):\n",
    "        epoch = i\n",
    "        print(f\"Epoch: {i:02d}, Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "        train_model(model, loader, optimizer)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            if i == 15:\n",
    "                print(\"got here\")\n",
    "\n",
    "                to_device(model.eval())\n",
    "                inputs, targets = loader\n",
    "                for batch_idx, (inps, targs) in enumerate(zip(inputs, targets), 0):\n",
    "                    inps = torch.from_numpy(inps)\n",
    "                    inps = to_device(inps)\n",
    "                    preds = model(inps)\n",
    "                    pred = nn.Softmax(dim=1)(preds)\n",
    "                    pred_labels = pred.argmax(dim=1)\n",
    "                    # Add a value 1 dimension at dim=1\n",
    "                    pred_labels = pred_labels.unsqueeze(1)\n",
    "                    # print(\"pred_labels.shape: {}\".format(pred_labels.shape))\n",
    "                    pred_mask = pred_labels.to(torch.float)\n",
    "                    print(\"PREDS:\", torch.unique(pred_mask))\n",
    "                    print(\"PREDS\", pred_mask.shape)\n",
    "                    print(type(pred_mask))\n",
    "                    ones_indices = torch.nonzero(pred_mask == 1)\n",
    "\n",
    "                    inspect = pred_mask[0,0,0:10,0:10]\n",
    "                    print(\"ONES:\", ones_indices)\n",
    "                break\n",
    "        #     print_test_dataset_masks(model, test_inputs, test_targets, epoch=epoch, save_path=save_path, show_plot=True)\n",
    "        # # end with\n",
    "        # with torch.inference_mode():\n",
    "        #     # Display the plt in the final training epoch.\n",
    "        #     # (epoch == epoch_j-1)\n",
    "        #     print_test_dataset_masks(model, test_inputs, test_targets, epoch=epoch, save_path=save_path, show_plot=True)\n",
    "        # # end with\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        # end if\n",
    "        print(\"\")\n",
    "    # end for\n",
    "# end def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = vit\n",
    "images_folder_name = \"vit_training_progress_images\"\n",
    "save_path = os.path.join('output', images_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Learning Rate Scheduler.\n",
    "to_device(m)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.0004)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=12, gamma=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original:\n",
    "\"\"\"\n",
    "train_loop(m, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 51), optimizer, scheduler, save_path=save_path)\n",
    "m = model\n",
    "pets_train_loader = torch dataloader object\n",
    "We will split this into a inputs and targets vars for the sake of fast implementation.\n",
    "\n",
    "test_pets_inputs = Used for unnecesary things and removed from training loop\n",
    "test_pets_targets = Used for unnecesary things and removed from training loop\n",
    "\"\"\"\n",
    "\n",
    "test_pets_inputs = None\n",
    "test_pets_targets = None\n",
    "\n",
    "# Custom:\n",
    "# train_loop(m, (inputs, targets), (test_pets_inputs, test_pets_targets), (1, 51), optimizer, scheduler, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 128])\n",
      "(64, 3, 128, 128)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms \n",
    "from PIL import ImageOps\n",
    "\n",
    "img_size_custom = 128\n",
    "\n",
    "inputs = Image.open(\"../raw_data/test/dikke_kippen/kip2.jpg\")\n",
    "\n",
    "inputs = ImageOps.grayscale(inputs) \n",
    "\n",
    "segmask = np.load(\"../dataset/RibFrac421-image/frac_0/pos_label/pos-slice-0-label.npy\")\n",
    "segmask = torch.from_numpy(segmask)\n",
    "img2tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "\n",
    "img_tensor = img2tensor(inputs)\n",
    "img_tensor = img_tensor[:, 0:128, 0:128]\n",
    "\n",
    "print(img_tensor.shape)\n",
    "\n",
    "\n",
    "fractures = 64\n",
    "\n",
    "inputs_batch = torch.zeros((fractures, 3, img_size_custom, img_size_custom))\n",
    "for i in range(len(fractures)):\n",
    "    # Get image 64 + 64 + 64 + 64\n",
    "    inputs_batch[i,:,:,:] = img_tensor\n",
    "\n",
    "tmp = torch.zeros((img_size_custom, img_size_custom))\n",
    "tmp[0:64, 0:64] = segmask\n",
    "segmask_batch = torch.zeros((fractures, img_size_custom, img_size_custom))\n",
    "for i in range(64):\n",
    "    segmask_batch[i,:,:] = tmp\n",
    "\n",
    "inputs_batch = inputs_batch.numpy()\n",
    "segmask_batch = segmask_batch.numpy()\n",
    "\n",
    "print(inputs_batch.shape)\n",
    "# print(inputs_batch.shape)\n",
    "# sys.exit()\n",
    "\n",
    "print(type(inputs_batch))\n",
    "print(type(inputs_batch[0]))\n",
    "\n",
    "\n",
    "# train_loop(m, ([inputs_batch], [segmask_batch]), (test_pets_inputs, test_pets_targets), (1, 51), optimizer, scheduler, save_path=save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
